{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc63666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"子供は公園で遊んだり、家で勉強したりします。\"\n",
    "text = \"食べさせられました。この車は速くありませんでした。怒ってしまった。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef132097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc4667",
   "metadata": {},
   "source": [
    "# SudachiPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sudachipy import dictionary, tokenizer\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary(dict=\"full\").create()\n",
    "mode = tokenizer.Tokenizer.SplitMode.C\n",
    "\n",
    "morphemes = tokenizer_obj.tokenize(text, mode)\n",
    "\n",
    "data = []\n",
    "for m in morphemes:\n",
    "    data.append({\n",
    "        \"surface\": m.surface(),\n",
    "        \"dictionary_form\": m.dictionary_form(),\n",
    "        \"normalized_form\": m.normalized_form(),\n",
    "        \"part_of_speech\": m.part_of_speech(),\n",
    "        \"reading_form\": m.reading_form(),\n",
    "        \"begin\": m.begin(),\n",
    "        \"end\": m.end(),\n",
    "        \"is_oov\": m.is_oov(),\n",
    "        \"dictionary_id\": m.dictionary_id(),\n",
    "        \"part_of_speech_id\": m.part_of_speech_id(),\n",
    "        \"word_id\": m.word_id(),\n",
    "        \"synonym_group_ids\": m.synonym_group_ids(),\n",
    "        \"raw_surface\": m.raw_surface()\n",
    "    })\n",
    "\n",
    "# print(tabulate(pd.DataFrame(data), headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16054c8c",
   "metadata": {},
   "source": [
    "# Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "from nltk import Tree\n",
    "\n",
    "nlp = stanza.Pipeline(lang=\"ja\", processors='tokenize, lemma, pos, constituency', model_dir=f\"../toy_content/_stanza_resources\", logging_level='WARN')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    tokens = [word.to_dict() for word in sentence.tokens]\n",
    "    print(sum(tokens, []))\n",
    "\n",
    "    display(pd.DataFrame(sum(tokens, [])))\n",
    "    # print(tabulate(pd.DataFrame(sum(tokens, [])), headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "    tree = Tree.fromstring(str(sentence.constituency))\n",
    "    tree.pretty_print()   # Pretty text-based tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a80bd7",
   "metadata": {},
   "source": [
    "# Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d224bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "result = subprocess.run(['mecab'], input=text, text=True, capture_output=True)\n",
    "lines = result.stdout.strip().split('\\n')\n",
    "rows = []\n",
    "for line in lines:\n",
    "    if line == 'EOS' or not line:\n",
    "        continue\n",
    "    cols = line.split('\\t')\n",
    "    if len(cols) < 2:\n",
    "        continue\n",
    "    surface = cols[0]\n",
    "    features = cols[1].split(',')\n",
    "    row = {\n",
    "        \"surface\": surface,\n",
    "        \"pos\": features[0] if len(features) > 0 else \"\",\n",
    "        \"pos_detail1\": features[1] if len(features) > 1 else \"\",\n",
    "        \"pos_detail2\": features[2] if len(features) > 2 else \"\",\n",
    "        \"pos_detail3\": features[3] if len(features) > 3 else \"\",\n",
    "        \"conjugation_form\": features[4] if len(features) > 4 else \"\",\n",
    "        \"conjugation_type\": features[5] if len(features) > 5 else \"\",\n",
    "        \"base_form\": features[6] if len(features) > 6 else \"\",\n",
    "        \"reading\": features[7] if len(features) > 7 else \"\",\n",
    "        \"pronunciation\": features[8] if len(features) > 8 else \"\",\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "display(pd.DataFrame(rows))\n",
    "print(tabulate(pd.DataFrame(rows), headers=\"keys\", tablefmt=\"github\", showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d802e994",
   "metadata": {},
   "source": [
    "# Fugashi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f7d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fugashi\n",
    "import unidic\n",
    "tagger = fugashi.Tagger('-d ' + unidic.DICDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.parse(text)\n",
    "for word in tagger(text):\n",
    "    print(word, word.feature.lemma, word.pos, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025614c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_verb_conjugation(text):\n",
    "    words = list(tagger(text))\n",
    "    analysis = {\n",
    "        \"input\": text,\n",
    "        \"dictionary_form\": None,\n",
    "        \"conjugation_chain\": [],\n",
    "        \"segments\": []\n",
    "    }\n",
    "\n",
    "    for word in words:\n",
    "        pos = word.pos\n",
    "        lemma = word.feature.lemma\n",
    "        inflection_type = word.feature.cType  # 活用型\n",
    "        inflection_form = word.feature.cForm  # 活用形\n",
    "\n",
    "        if '動詞' in pos or '助動詞' in pos:\n",
    "            # Add to conjugation chain\n",
    "            segment_info = {\n",
    "                \"surface\": word.surface,\n",
    "                \"lemma\": lemma,\n",
    "                \"pos\": pos,\n",
    "                \"conjugation_type\": inflection_type,\n",
    "                \"conjugation_form\": inflection_form\n",
    "            }\n",
    "            analysis[\"segments\"].append(segment_info)\n",
    "\n",
    "            if analysis[\"dictionary_form\"] is None:\n",
    "                analysis[\"dictionary_form\"] = lemma\n",
    "\n",
    "            if inflection_form and inflection_form != \"*\":\n",
    "                analysis[\"conjugation_chain\"].append({\n",
    "                    \"type\": inflection_type,\n",
    "                    \"form\": inflection_form,\n",
    "                    \"surface\": word.surface\n",
    "                })\n",
    "\n",
    "    return analysis\n",
    "\n",
    "# Test examples\n",
    "test_inputs = [\n",
    "    \"食べさせられました\",   # causative passive past polite\n",
    "    \"読まなかった\",         # negative past\n",
    "    \"行っている\",           # te-form + progressive\n",
    "    \"見られる\",             # potential/passive\n",
    "    \"書かせていただきます\" # causative + humble polite\n",
    "]\n",
    "\n",
    "for text in test_inputs:\n",
    "    print(\"=\" * 50)\n",
    "    result = analyze_verb_conjugation(text)\n",
    "    print(f\"Input: {result['input']}\")\n",
    "    print(f\"Dictionary form: {result['dictionary_form']}\")\n",
    "    print(\"Conjugation chain:\")\n",
    "    for c in result[\"conjugation_chain\"]:\n",
    "        print(f\"  - {c['surface']} ({c['type']} - {c['form']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d63f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fugashi\n",
    "import unidic\n",
    "\n",
    "tagger = fugashi.Tagger('-d ' + unidic.DICDIR)\n",
    "text = \"食べさせられました\"\n",
    "\n",
    "words = list(tagger(text))\n",
    "\n",
    "# Base dictionary form and first surface\n",
    "dictionary_form = words[0].feature.lemma\n",
    "\n",
    "# Map auxiliaries to grammatical functions\n",
    "AUX_MEANINGS = {\n",
    "    'させる': 'causative',\n",
    "    'られる': 'passive',\n",
    "    'ます': 'polite',\n",
    "    'た': 'past',\n",
    "    'ない': 'negative',\n",
    "    'たい': 'desire',\n",
    "    'う': 'volitional',\n",
    "}\n",
    "\n",
    "# Start with empty conjugated string\n",
    "conjugated = \"\"\n",
    "chain = []\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    surface = word.surface\n",
    "    lemma = word.feature.lemma\n",
    "    cType = word.feature.cType\n",
    "    cForm = word.feature.cForm\n",
    "\n",
    "    conjugated += surface\n",
    "\n",
    "    # Find grammatical function if auxiliary (except main verb)\n",
    "    gram_func = \"\"\n",
    "    if i > 0 and lemma in AUX_MEANINGS:\n",
    "        gram_func = AUX_MEANINGS[lemma]\n",
    "\n",
    "    chain.append({\n",
    "        'conjugated': conjugated,\n",
    "        'surface': surface,\n",
    "        'lemma': lemma,\n",
    "        'cType': cType,\n",
    "        'cForm': cForm,\n",
    "        'function': gram_func,\n",
    "    })\n",
    "\n",
    "# Output\n",
    "print(f\"Input: {text}\")\n",
    "print(f\"Dictionary form: {dictionary_form}\")\n",
    "print(\"Conjugation chain with cumulative forms:\")\n",
    "\n",
    "for step in chain:\n",
    "    func_str = f\" - {step['function']}\" if step['function'] else \"\"\n",
    "    print(f\"  - {step['conjugated']} ({step['lemma']} - {step['cType']} - {step['cForm']}){func_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741cff91",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pykakasi\n",
    "japanese_nlp = spacy.load(\"ja_core_news_sm\", disable = ['ner', 'parser'])\n",
    "# japanese_nlp.add_pipe(\"custom_sentence_splitter\", first=True)\n",
    "japanese_nlp.add_pipe('sentencizer')\n",
    "hiraganaConverter = pykakasi.kakasi()\n",
    "\n",
    "doc = japanese_nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502cd3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentenceIndex, sentence in enumerate(doc.sents):\n",
    "    for token in sentence:\n",
    "        print(\"---\")\n",
    "        print(token)\n",
    "        print(token.lemma_)\n",
    "\n",
    "        reading = list()\n",
    "        lemmaReading = list()\n",
    "        if True or language == 'japanese':\n",
    "            result = hiraganaConverter.convert(token.text)\n",
    "            for x in result:\n",
    "                reading.append(x['hira'])\n",
    "            \n",
    "            result = hiraganaConverter.convert(token.lemma_)\n",
    "            for x in result:\n",
    "                lemmaReading.append(x['hira'])\n",
    "        \n",
    "            reading = ''.join(reading)\n",
    "            lemmaReading = ''.join(lemmaReading)\n",
    "            \n",
    "        print(reading)\n",
    "        print(lemmaReading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb9fb3",
   "metadata": {},
   "source": [
    "# Ginza via Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d569ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')\n",
    "doc = nlp(text)\n",
    "\n",
    "# available token attributes https://spacy.io/api/token#attributes\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print(\n",
    "            token.i,\n",
    "            token.orth_,\n",
    "            token.lemma_,\n",
    "            token.norm_,\n",
    "            token.text,\n",
    "            token.head,\n",
    "            token.morph.get(\"Reading\"),\n",
    "            token.pos_,\n",
    "            token.morph.get(\"Inflection\"),\n",
    "            token.tag_,\n",
    "            token.dep_,\n",
    "            token.head.i,\n",
    "        )\n",
    "    print('EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in nlp.pipeline:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98112725",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d13d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for p in nlp.pipeline:\n",
    "    print(p)\n",
    "    \n",
    "def on_match(matcher, doc, id, matches):\n",
    "    print('Matched!', matches)\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"OBAMA\", [nlp(\"Barack Obama\")], on_match=on_match)\n",
    "matcher.add(\"OBAMAU\", [nlp(\"Barack Obama urges\")], on_match=on_match)\n",
    "matcher.add(\"FAREWELL\", [nlp(\"emotional farewell\"), nlp(\"emotional farewells\")], on_match=on_match)\n",
    "doc = nlp(\"Barack Obama lifts America one last time in emotional farewell\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "influx_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
