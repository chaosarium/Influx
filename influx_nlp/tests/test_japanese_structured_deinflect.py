from __future__ import annotations
from inline_snapshot import snapshot
from lib.annotation import ParserConfig
from lib.parsing import JapaneseParser, SpacyParser
from lib.japanese_deinflect.deinflect import Deinflector
from lib.japanese_deinflect.word_type import WordType
from lib.japanese_deinflect.derivations import rules
from lib.annotation import *

deinflector = Deinflector()
parser = JapaneseParser()


def test_ambiguous_deinflections():
    """Test cases with ambiguous deinflections that return multiple possibilities."""

    results_itte = deinflector.unconjugate('ã„ã£ã¦')
    assert len(results_itte) == snapshot(8)
    assert results_itte == snapshot(
        [
            {'base': 'ã„ã†', 'derivation_sequence': {'derivations': [WordType.TE_FORM], 'word_form_progression': ['ã„ã£ã¦']}},
            {'base': 'ã„ã¤', 'derivation_sequence': {'derivations': [WordType.TE_FORM], 'word_form_progression': ['ã„ã£ã¦']}},
            {'base': 'ã„ã‚‹', 'derivation_sequence': {'derivations': [WordType.TE_FORM], 'word_form_progression': ['ã„ã£ã¦']}},
            {'base': 'ã„ã£ã¦', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã„ã£ã‚‹', 'derivation_sequence': {'derivations': [WordType.TE_FORM], 'word_form_progression': ['ã„ã£ã¦']}},
            {'base': 'ã„ã£ã¤', 'derivation_sequence': {'derivations': [WordType.IMPERATIVE], 'word_form_progression': ['ã„ã£ã¦']}},
            {'base': 'ã„ã£ã¤', 'derivation_sequence': {'derivations': [WordType.POTENTIAL, WordType.MASU_STEM], 'word_form_progression': ['ã„ã£ã¦ã‚‹', 'ã„ã£ã¦']}},
            {'base': 'ã„ã£ã¦ã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã„ã£ã¦']}},
        ]
    )

    results_shita = deinflector.unconjugate('ã—ãŸ')
    assert len(results_shita) == snapshot(5)
    assert results_shita == snapshot(
        [
            {'base': 'ã™ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã—ãŸ']}},
            {'base': 'ã™', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã—ãŸ']}},
            {'base': 'ã—ãŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã—ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã—ãŸ']}},
            {'base': 'ã—ãŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã—ãŸ']}},
        ]
    )

    results_aru = deinflector.unconjugate('ã‚ã‚‹')
    assert len(results_aru) == snapshot(2)
    assert results_aru == snapshot(
        [
            {'base': 'ã‚ã‚‹', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã‚ã‚‹ã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã‚ã‚‹']}},
        ]
    )

    results_iru = deinflector.unconjugate('ã„ã‚‹')
    assert len(results_iru) == snapshot(2)
    assert results_iru == snapshot(
        [
            {'base': 'ã„ã‚‹', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã„ã‚‹ã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã„ã‚‹']}},
        ]
    )

    results_kita = deinflector.unconjugate('ããŸ')
    assert len(results_kita) == snapshot(3)
    assert results_kita == snapshot(
        [
            {'base': 'ããŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ãã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ããŸ']}},
            {'base': 'ããŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ããŸ']}},
        ]
    )

    results_katta = deinflector.unconjugate('ã‹ã£ãŸ')
    assert len(results_katta) == snapshot(6)
    assert results_katta == snapshot(
        [
            {'base': 'ã‹ã†', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã‹ã£ãŸ']}},
            {'base': 'ã‹ã¤', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã‹ã£ãŸ']}},
            {'base': 'ã‹ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã‹ã£ãŸ']}},
            {'base': 'ã‹ã£ãŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã‹ã£ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã‹ã£ãŸ']}},
            {'base': 'ã‹ã£ãŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã‹ã£ãŸ']}},
        ]
    )

    results_totta = deinflector.unconjugate('å–ã£ãŸ')
    assert len(results_totta) == snapshot(6)
    assert results_totta == snapshot(
        [
            {'base': 'å–ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['å–ã£ãŸ']}},
            {'base': 'å–ã†', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['å–ã£ãŸ']}},
            {'base': 'å–ã¤', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['å–ã£ãŸ']}},
            {'base': 'å–ã£ãŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'å–ã£ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['å–ã£ãŸ']}},
            {'base': 'å–ã£ãŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['å–ã£ãŸ']}},
        ]
    )


def test_ambiguity_resolution_1():
    """Deinflection filters by what spacy thinks."""
    # TODO test WIP
    result = deinflector.unconjugate('æ®ºã•ã‚Œã‚‹ãª')
    assert len(result) == snapshot(11)
    assert result == snapshot(
        [
            {'base': 'æ®ºã™', 'derivation_sequence': {'derivations': [WordType.PASSIVE, WordType.NA_COMMAND], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã™', 'derivation_sequence': {'derivations': [WordType.PASSIVE, WordType.NA_PARTICLE], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã™ã‚‹', 'derivation_sequence': {'derivations': [WordType.PASSIVE, WordType.NA_COMMAND], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã™ã‚‹', 'derivation_sequence': {'derivations': [WordType.PASSIVE, WordType.NA_PARTICLE], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã•ã‚‹', 'derivation_sequence': {'derivations': [WordType.POTENTIAL, WordType.NA_COMMAND], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã•ã‚‹', 'derivation_sequence': {'derivations': [WordType.POTENTIAL, WordType.NA_PARTICLE], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã•ã‚Œã‚‹', 'derivation_sequence': {'derivations': [WordType.NA_COMMAND], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã•ã‚Œã‚‹', 'derivation_sequence': {'derivations': [WordType.NA_PARTICLE], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã•ã‚Œã‚‹ãª', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'æ®ºã•ã‚Œã‚‹ã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM, WordType.NASAI], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã•ã‚Œã‚‹ãªã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹ãª']}},
        ]
    )

    text = "æ®ºã•ã‚Œã‚‹ãª"
    result = parser.parse(text, ParserConfig("enhanced_japanese", {})).segments
    assert result == snapshot(
        [
            DocSegV2(
                text='æ®ºã•ã‚Œã‚‹ãª',
                start_char=0,
                end_char=5,
                inner=DocSegSentence(
                    segments=[
                        SentSegV2(
                            sentence_idx=0,
                            text='æ®ºã•',
                            start_char=0,
                            end_char=2,
                            inner=SentSegTokenSeg(idx=0, orthography='æ®ºã•'),
                            attributes=SegAttribute(
                                lemma='æ®ºã™',
                                upos='VERB',
                                xpos='å‹•è©-ä¸€èˆ¬',
                                dependency=(0, 'ROOT'),
                                misc={'Inflection': 'äº”æ®µ-ã‚µè¡Œ;æœªç„¶å½¢-ä¸€èˆ¬', 'Reading': 'ã‚³ãƒ­ã‚µ', 'furigana_bracket': 'æ®º[ã“ã‚]ã•', 'furigana_ruby': '<ruby>æ®º<rt>ã“ã‚</rt></ruby>ã•', 'furigana_parentheses': 'æ®º(ã“ã‚)ã•', 'hiragana_reading': 'ã“ã‚ã•'},
                            ),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ã‚Œã‚‹',
                            start_char=2,
                            end_char=4,
                            inner=SentSegTokenSeg(idx=1, orthography='ã‚Œã‚‹'),
                            attributes=SegAttribute(lemma='ã‚Œã‚‹', upos='AUX', xpos='åŠ©å‹•è©', dependency=(0, 'aux'), misc={'Inflection': 'åŠ©å‹•è©-ãƒ¬ãƒ«;çµ‚æ­¢å½¢-ä¸€èˆ¬', 'Reading': 'ãƒ¬ãƒ«', 'furigana_bracket': 'ã‚Œã‚‹', 'furigana_ruby': 'ã‚Œã‚‹', 'furigana_parentheses': 'ã‚Œã‚‹', 'hiragana_reading': 'ã‚Œã‚‹'}),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ãª',
                            start_char=4,
                            end_char=5,
                            inner=SentSegTokenSeg(idx=2, orthography='ãª'),
                            attributes=SegAttribute(lemma='ãª', upos='PART', xpos='åŠ©è©-çµ‚åŠ©è©', dependency=(0, 'mark'), misc={'Reading': 'ãƒŠ', 'furigana_bracket': 'ãª', 'furigana_ruby': 'ãª', 'furigana_parentheses': 'ãª', 'hiragana_reading': 'ãª'}),
                        ),
                    ]
                ),
            )
        ]
    )


def test_ambiguity_resolution_2():
    """Deinflection filters by what spacy thinks."""
    # TODO test WIP
    text = 'å­¦æ ¡ã«ã„ã£ãŸã€‚'
    verb = 'ã„ã£ãŸ'
    result = deinflector.unconjugate(verb)
    assert len(result) == snapshot(6)
    # BUG doesn't even include è¡Œã (ğ–¦¹ï¹ğ–¦¹;)
    assert result == snapshot(
        [
            {'base': 'ã„ã†', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã¤', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã£ãŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã„ã£ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã£ãŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã„ã£ãŸ']}},
        ]
    )

    # result = SpacyParser().parse(text, ParserConfig("spacy", {"spacy_model": "ja_core_news_sm"})).segments
    result = parser.parse(text, ParserConfig("testing", {})).segments
    assert result == snapshot(
        [
            DocSegV2(
                text='å­¦æ ¡ã«ã„ã£ãŸã€‚',
                start_char=0,
                end_char=7,
                inner=DocSegSentence(
                    segments=[
                        SentSegV2(
                            sentence_idx=0,
                            text='å­¦æ ¡',
                            start_char=0,
                            end_char=2,
                            inner=SentSegTokenSeg(idx=0, orthography='å­¦æ ¡'),
                            attributes=SegAttribute(
                                lemma='å­¦æ ¡',
                                upos='NOUN',
                                xpos='åè©-æ™®é€šåè©-ä¸€èˆ¬',
                                dependency=(2, 'obl'),
                                misc={'Reading': 'ã‚¬ãƒƒã‚³ã‚¦', 'furigana_bracket': 'å­¦æ ¡[ãŒã£ã“ã†]', 'furigana_ruby': '<ruby>å­¦æ ¡<rt>ãŒã£ã“ã†</rt></ruby>', 'furigana_parentheses': 'å­¦æ ¡(ãŒã£ã“ã†)', 'hiragana_reading': 'ãŒã£ã“ã†'},
                            ),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ã«',
                            start_char=2,
                            end_char=3,
                            inner=SentSegTokenSeg(idx=1, orthography='ã«'),
                            attributes=SegAttribute(lemma='ã«', upos='ADP', xpos='åŠ©è©-æ ¼åŠ©è©', dependency=(0, 'case'), misc={'Reading': 'ãƒ‹', 'furigana_bracket': 'ã«', 'furigana_ruby': 'ã«', 'furigana_parentheses': 'ã«', 'hiragana_reading': 'ã«'}),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ã„ã£',
                            start_char=3,
                            end_char=5,
                            inner=SentSegTokenSeg(idx=2, orthography='ã„ã£'),
                            attributes=SegAttribute(
                                lemma='ã„ã', upos='VERB', xpos='å‹•è©-éè‡ªç«‹å¯èƒ½', dependency=(2, 'ROOT'), misc={'Inflection': 'äº”æ®µ-ã‚«è¡Œ;é€£ç”¨å½¢-ä¿ƒéŸ³ä¾¿', 'Reading': 'ã‚¤ãƒƒ', 'furigana_bracket': 'ã„ã£', 'furigana_ruby': 'ã„ã£', 'furigana_parentheses': 'ã„ã£', 'hiragana_reading': 'ã„ã£'}
                            ),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ãŸ',
                            start_char=5,
                            end_char=6,
                            inner=SentSegTokenSeg(idx=3, orthography='ãŸ'),
                            attributes=SegAttribute(lemma='ãŸ', upos='AUX', xpos='åŠ©å‹•è©', dependency=(2, 'aux'), misc={'Inflection': 'åŠ©å‹•è©-ã‚¿;çµ‚æ­¢å½¢-ä¸€èˆ¬', 'Reading': 'ã‚¿', 'furigana_bracket': 'ãŸ', 'furigana_ruby': 'ãŸ', 'furigana_parentheses': 'ãŸ', 'hiragana_reading': 'ãŸ'}),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ã€‚',
                            start_char=6,
                            end_char=7,
                            inner=SentSegPunctuationSeg(),
                            attributes=SegAttribute(lemma='ã€‚', upos='PUNCT', xpos='è£œåŠ©è¨˜å·-å¥ç‚¹', dependency=(2, 'punct'), misc={'Reading': 'ã€‚', 'furigana_bracket': 'ã€‚', 'furigana_ruby': 'ã€‚', 'furigana_parentheses': 'ã€‚', 'hiragana_reading': 'ã€‚'}),
                        ),
                    ]
                ),
            )
        ]
    )

    text = 'å…ˆç”ŸãŒãã†ã„ã£ãŸã€‚' # è¨€ã†
    result = deinflector.unconjugate(verb)
    assert len(result) == snapshot(6)
    assert result == snapshot(
        [
            {'base': 'ã„ã†', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã¤', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã£ãŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã„ã£ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã£ãŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã„ã£ãŸ']}},
        ]
    )

    result = parser.parse(text, ParserConfig("testing", {})).segments
    assert result == snapshot(
        [
            DocSegV2(
                text='å…ˆç”ŸãŒãã†ã„ã£ãŸã€‚',
                start_char=0,
                end_char=9,
                inner=DocSegSentence(
                    segments=[
                        SentSegV2(
                            sentence_idx=0,
                            text='å…ˆç”Ÿ',
                            start_char=0,
                            end_char=2,
                            inner=SentSegTokenSeg(idx=0, orthography='å…ˆç”Ÿ'),
                            attributes=SegAttribute(
                                lemma='å…ˆç”Ÿ',
                                upos='NOUN',
                                xpos='åè©-æ™®é€šåè©-ä¸€èˆ¬',
                                dependency=(3, 'nsubj'),
                                misc={'Reading': 'ã‚»ãƒ³ã‚»ã‚¤', 'furigana_bracket': 'å…ˆç”Ÿ[ã›ã‚“ã›ã„]', 'furigana_ruby': '<ruby>å…ˆç”Ÿ<rt>ã›ã‚“ã›ã„</rt></ruby>', 'furigana_parentheses': 'å…ˆç”Ÿ(ã›ã‚“ã›ã„)', 'hiragana_reading': 'ã›ã‚“ã›ã„'},
                            ),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ãŒ',
                            start_char=2,
                            end_char=3,
                            inner=SentSegTokenSeg(idx=1, orthography='ãŒ'),
                            attributes=SegAttribute(lemma='ãŒ', upos='ADP', xpos='åŠ©è©-æ ¼åŠ©è©', dependency=(0, 'case'), misc={'Reading': 'ã‚¬', 'furigana_bracket': 'ãŒ', 'furigana_ruby': 'ãŒ', 'furigana_parentheses': 'ãŒ', 'hiragana_reading': 'ãŒ'}),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ãã†',
                            start_char=3,
                            end_char=5,
                            inner=SentSegTokenSeg(idx=2, orthography='ãã†'),
                            attributes=SegAttribute(lemma='ãã†', upos='ADV', xpos='å‰¯è©', dependency=(3, 'advmod'), misc={'Reading': 'ã‚½ã‚¦', 'furigana_bracket': 'ãã†', 'furigana_ruby': 'ãã†', 'furigana_parentheses': 'ãã†', 'hiragana_reading': 'ãã†'}),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ã„ã£',
                            start_char=5,
                            end_char=7,
                            inner=SentSegTokenSeg(idx=3, orthography='ã„ã£'),
                            attributes=SegAttribute(
                                lemma='ã„ã†', upos='VERB', xpos='å‹•è©-ä¸€èˆ¬', dependency=(3, 'ROOT'), misc={'Inflection': 'äº”æ®µ-ãƒ¯ã‚¢è¡Œ;é€£ç”¨å½¢-ä¿ƒéŸ³ä¾¿', 'Reading': 'ã‚¤ãƒƒ', 'furigana_bracket': 'ã„ã£', 'furigana_ruby': 'ã„ã£', 'furigana_parentheses': 'ã„ã£', 'hiragana_reading': 'ã„ã£'}
                            ),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ãŸ',
                            start_char=7,
                            end_char=8,
                            inner=SentSegTokenSeg(idx=4, orthography='ãŸ'),
                            attributes=SegAttribute(lemma='ãŸ', upos='AUX', xpos='åŠ©å‹•è©', dependency=(3, 'aux'), misc={'Inflection': 'åŠ©å‹•è©-ã‚¿;çµ‚æ­¢å½¢-ä¸€èˆ¬', 'Reading': 'ã‚¿', 'furigana_bracket': 'ãŸ', 'furigana_ruby': 'ãŸ', 'furigana_parentheses': 'ãŸ', 'hiragana_reading': 'ãŸ'}),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ã€‚',
                            start_char=8,
                            end_char=9,
                            inner=SentSegPunctuationSeg(),
                            attributes=SegAttribute(lemma='ã€‚', upos='PUNCT', xpos='è£œåŠ©è¨˜å·-å¥ç‚¹', dependency=(3, 'punct'), misc={'Reading': 'ã€‚', 'furigana_bracket': 'ã€‚', 'furigana_ruby': 'ã€‚', 'furigana_parentheses': 'ã€‚', 'hiragana_reading': 'ã€‚'}),
                        ),
                    ]
                ),
            )
        ]
    )


def analyze_word(word: str) -> str:
    results = deinflector.unconjugate(word)

    if not results:
        return f"No results found for '{word}'"

    output = []
    output.append(f"Analysis of '{word}':")
    output.append(f"Dictionary form: {results[0]['base']}")
    output.append("")

    derivations = results[0]["derivation_sequence"]["derivations"]
    word_progression = results[0]["derivation_sequence"]["word_form_progression"]

    if not derivations:
        output.append("This is already in dictionary form.")
        return "\n".join(output)

    output.append("Derivation steps:")
    current_word = results[0]["base"]

    for i, (derivation_type, next_word) in enumerate(zip(derivations, word_progression)):

        output.append(f"{i+1}. {current_word} â†’ {next_word}")
        output.append(f"   Form: {derivation_type.value}")

        current_word = next_word
        output.append("")

    return "\n".join(output)


def test_analysis():
    """Test that grammar explanations are generated correctly for example words."""
    example_words = [
        "è¡Œãã¾ã™",
        "é£Ÿã¹ã‚‰ã‚Œãªã„",
        "èª­ã‚“ã§ã„ãŸ",
        "æ›¸ã‹ã‚Œã‚‹",
        "é£²ã¿ãŸã„",
    ]

    output_parts = []
    for word in example_words:
        analysis = analyze_word(word)
        output_parts.append(analysis)
        output_parts.append("-" * 50)
        output_parts.append("")

    if output_parts:
        output_parts = output_parts[:-2]

    combined_output = "\n".join(output_parts)
    assert combined_output == snapshot(
        """\
Analysis of 'è¡Œãã¾ã™':
Dictionary form: è¡Œã

Derivation steps:
1. è¡Œã â†’ è¡Œã
   Form: ã¾ã™ Stem

2. è¡Œã â†’ è¡Œãã¾ã™
   Form: ã¾ã™ Polite

--------------------------------------------------

Analysis of 'é£Ÿã¹ã‚‰ã‚Œãªã„':
Dictionary form: é£Ÿã¹ã‚‹

Derivation steps:
1. é£Ÿã¹ã‚‹ â†’ é£Ÿã¹ã‚‰ã‚Œã‚‹
   Form: Potential Or Passive Form

2. é£Ÿã¹ã‚‰ã‚Œã‚‹ â†’ é£Ÿã¹ã‚‰ã‚Œãªã„
   Form: ãªã„ Negative

--------------------------------------------------

Analysis of 'èª­ã‚“ã§ã„ãŸ':
Dictionary form: èª­ã‚€

Derivation steps:
1. èª­ã‚€ â†’ èª­ã‚“ã§
   Form: ã¦ãƒ»ã§ Form

2. èª­ã‚“ã§ â†’ èª­ã‚“ã§ã„ã‚‹
   Form: ã¦ã„ã‚‹ãƒ»ã§ã„ã‚‹ Continuing State/Result

3. èª­ã‚“ã§ã„ã‚‹ â†’ èª­ã‚“ã§ã„ãŸ
   Form: Plain Past

--------------------------------------------------

Analysis of 'æ›¸ã‹ã‚Œã‚‹':
Dictionary form: æ›¸ã

Derivation steps:
1. æ›¸ã â†’ æ›¸ã‹ã‚Œã‚‹
   Form: Passive Form

--------------------------------------------------

Analysis of 'é£²ã¿ãŸã„':
Dictionary form: é£²ã‚€

Derivation steps:
1. é£²ã‚€ â†’ é£²ã¿
   Form: ã¾ã™ Stem

2. é£²ã¿ â†’ é£²ã¿ãŸã„
   Form: ãŸã„ Want To Do
"""
    )
