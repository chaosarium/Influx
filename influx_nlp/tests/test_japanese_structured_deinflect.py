from __future__ import annotations
from inline_snapshot import snapshot
from lib.annotation import ParserConfig
from lib.parsing import JapaneseParser, SpacyParser
from lib.japanese_deinflect.deinflect import Deinflector
from lib.japanese_deinflect.word_type import WordType
from lib.japanese_deinflect.derivations import rules
from lib.annotation import *
from lib.japanese_conjugation_analysis import JapaneseConjugationAnalyzer

deinflector = Deinflector()
parser = JapaneseParser()
conjugation_analyzer = JapaneseConjugationAnalyzer()


def test_ambiguous_deinflections():
    """Test cases with ambiguous deinflections that return multiple possibilities."""

    results_itte = deinflector.unconjugate('ã„ã£ã¦')
    assert len(results_itte) == snapshot(8)
    assert results_itte == snapshot(
        [
            {'base': 'ã„ã†', 'derivation_sequence': {'derivations': [WordType.TE_FORM], 'word_form_progression': ['ã„ã£ã¦']}},
            {'base': 'ã„ã¤', 'derivation_sequence': {'derivations': [WordType.TE_FORM], 'word_form_progression': ['ã„ã£ã¦']}},
            {'base': 'ã„ã‚‹', 'derivation_sequence': {'derivations': [WordType.TE_FORM], 'word_form_progression': ['ã„ã£ã¦']}},
            {'base': 'ã„ã£ã¦', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã„ã£ã‚‹', 'derivation_sequence': {'derivations': [WordType.TE_FORM], 'word_form_progression': ['ã„ã£ã¦']}},
            {'base': 'ã„ã£ã¤', 'derivation_sequence': {'derivations': [WordType.IMPERATIVE], 'word_form_progression': ['ã„ã£ã¦']}},
            {'base': 'ã„ã£ã¤', 'derivation_sequence': {'derivations': [WordType.POTENTIAL, WordType.MASU_STEM], 'word_form_progression': ['ã„ã£ã¦ã‚‹', 'ã„ã£ã¦']}},
            {'base': 'ã„ã£ã¦ã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã„ã£ã¦']}},
        ]
    )

    results_shita = deinflector.unconjugate('ã—ãŸ')
    assert len(results_shita) == snapshot(5)
    assert results_shita == snapshot(
        [
            {'base': 'ã™ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã—ãŸ']}},
            {'base': 'ã™', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã—ãŸ']}},
            {'base': 'ã—ãŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã—ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã—ãŸ']}},
            {'base': 'ã—ãŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã—ãŸ']}},
        ]
    )

    results_aru = deinflector.unconjugate('ã‚ã‚‹')
    assert len(results_aru) == snapshot(2)
    assert results_aru == snapshot(
        [
            {'base': 'ã‚ã‚‹', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã‚ã‚‹ã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã‚ã‚‹']}},
        ]
    )

    results_iru = deinflector.unconjugate('ã„ã‚‹')
    assert len(results_iru) == snapshot(2)
    assert results_iru == snapshot(
        [
            {'base': 'ã„ã‚‹', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã„ã‚‹ã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã„ã‚‹']}},
        ]
    )

    results_kita = deinflector.unconjugate('ããŸ')
    assert len(results_kita) == snapshot(3)
    assert results_kita == snapshot(
        [
            {'base': 'ããŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ãã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ããŸ']}},
            {'base': 'ããŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ããŸ']}},
        ]
    )

    results_katta = deinflector.unconjugate('ã‹ã£ãŸ')
    assert len(results_katta) == snapshot(6)
    assert results_katta == snapshot(
        [
            {'base': 'ã‹ã†', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã‹ã£ãŸ']}},
            {'base': 'ã‹ã¤', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã‹ã£ãŸ']}},
            {'base': 'ã‹ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã‹ã£ãŸ']}},
            {'base': 'ã‹ã£ãŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã‹ã£ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã‹ã£ãŸ']}},
            {'base': 'ã‹ã£ãŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã‹ã£ãŸ']}},
        ]
    )

    results_totta = deinflector.unconjugate('å–ã£ãŸ')
    assert len(results_totta) == snapshot(6)
    assert results_totta == snapshot(
        [
            {'base': 'å–ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['å–ã£ãŸ']}},
            {'base': 'å–ã†', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['å–ã£ãŸ']}},
            {'base': 'å–ã¤', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['å–ã£ãŸ']}},
            {'base': 'å–ã£ãŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'å–ã£ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['å–ã£ãŸ']}},
            {'base': 'å–ã£ãŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['å–ã£ãŸ']}},
        ]
    )


def test_ambiguity_resolution_1():
    """Deinflection filters by what spacy thinks."""
    # TODO test WIP
    result = deinflector.unconjugate('æ®ºã•ã‚Œã‚‹ãª')
    assert len(result) == snapshot(11)
    assert result == snapshot(
        [
            {'base': 'æ®ºã™', 'derivation_sequence': {'derivations': [WordType.PASSIVE, WordType.NA_COMMAND], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã™', 'derivation_sequence': {'derivations': [WordType.PASSIVE, WordType.NA_PARTICLE], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã™ã‚‹', 'derivation_sequence': {'derivations': [WordType.PASSIVE, WordType.NA_COMMAND], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã™ã‚‹', 'derivation_sequence': {'derivations': [WordType.PASSIVE, WordType.NA_PARTICLE], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã•ã‚‹', 'derivation_sequence': {'derivations': [WordType.POTENTIAL, WordType.NA_COMMAND], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã•ã‚‹', 'derivation_sequence': {'derivations': [WordType.POTENTIAL, WordType.NA_PARTICLE], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã•ã‚Œã‚‹', 'derivation_sequence': {'derivations': [WordType.NA_COMMAND], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã•ã‚Œã‚‹', 'derivation_sequence': {'derivations': [WordType.NA_PARTICLE], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã•ã‚Œã‚‹ãª', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'æ®ºã•ã‚Œã‚‹ã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM, WordType.NASAI], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹', 'æ®ºã•ã‚Œã‚‹ãª']}},
            {'base': 'æ®ºã•ã‚Œã‚‹ãªã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['æ®ºã•ã‚Œã‚‹ãª']}},
        ]
    )

    text = "æ®ºã•ã‚Œã‚‹ãª"
    result = parser.parse(text, ParserConfig("enhanced_japanese", {})).segments
    assert result == snapshot(
        [
            DocSegV2(
                text='æ®ºã•ã‚Œã‚‹ãª',
                start_char=0,
                end_char=5,
                inner=DocSegSentence(
                    segments=[
                        SentSegV2(
                            sentence_idx=0,
                            text='æ®ºã•ã‚Œã‚‹ãª',
                            start_char=0,
                            end_char=5,
                            inner=SentSegTokenSeg(idx=0, orthography='æ®ºã•ã‚Œã‚‹ãª'),
                            attributes=SegAttribute(
                                lemma='æ®ºã™',
                                upos='VERB',
                                xpos='å‹•è©-ä¸€èˆ¬',
                                dependency=(0, 'ROOT'),
                                misc={
                                    'Inflection': 'äº”æ®µ-ã‚µè¡Œ;æœªç„¶å½¢-ä¸€èˆ¬',
                                    'Reading': 'ã‚³ãƒ­ã‚µ',
                                    'furigana_bracket': 'æ®º[ã“ã‚]ã•',
                                    'furigana_ruby': '<ruby>æ®º<rt>ã“ã‚</rt></ruby>ã•',
                                    'furigana_parentheses': 'æ®º(ã“ã‚)ã•',
                                    'hiragana_reading': 'ã“ã‚ã•',
                                    'conjugation_base': 'æ®ºã™',
                                    'conjugation_chain': [{'step': 1, 'form': 'ãª Negative Command (Do Not Do)', 'result': 'æ®ºã•ã‚Œã‚‹ãª'}, {'step': 2, 'form': 'Passive Form', 'result': 'æ®ºã•ã‚Œã‚‹'}],
                                    'conjugation_sequence_length': 3,
                                    'conjugation_combined_text': 'æ®ºã•ã‚Œã‚‹ãª',
                                },
                            ),
                        )
                    ]
                ),
            )
        ]
    )


def test_ambiguity_resolution_2():
    """Deinflection filters by what spacy thinks."""
    # TODO test WIP
    text = 'å­¦æ ¡ã«ã„ã£ãŸã€‚'
    verb = 'ã„ã£ãŸ'
    result = deinflector.unconjugate(verb)
    assert len(result) == snapshot(6)
    # BUG doesn't even include è¡Œã (ğ–¦¹ï¹ğ–¦¹;)
    assert result == snapshot(
        [
            {'base': 'ã„ã†', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã¤', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã£ãŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã„ã£ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã£ãŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã„ã£ãŸ']}},
        ]
    )

    # result = SpacyParser().parse(text, ParserConfig("spacy", {"spacy_model": "ja_core_news_sm"})).segments
    result = parser.parse(text, ParserConfig(which_parser="testing", parser_args={})).segments
    assert result == snapshot(
        [
            DocSegV2(
                text='å­¦æ ¡ã«ã„ã£ãŸã€‚',
                start_char=0,
                end_char=7,
                inner=DocSegSentence(
                    segments=[
                        SentSegV2(
                            sentence_idx=0,
                            text='å­¦æ ¡',
                            start_char=0,
                            end_char=2,
                            inner=SentSegTokenSeg(idx=0, orthography='å­¦æ ¡'),
                            attributes=SegAttribute(
                                lemma='å­¦æ ¡',
                                upos='NOUN',
                                xpos='åè©-æ™®é€šåè©-ä¸€èˆ¬',
                                dependency=(2, 'obl'),
                                misc={'Reading': 'ã‚¬ãƒƒã‚³ã‚¦', 'furigana_bracket': 'å­¦æ ¡[ãŒã£ã“ã†]', 'furigana_ruby': '<ruby>å­¦æ ¡<rt>ãŒã£ã“ã†</rt></ruby>', 'furigana_parentheses': 'å­¦æ ¡(ãŒã£ã“ã†)', 'hiragana_reading': 'ãŒã£ã“ã†'},
                            ),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ã«',
                            start_char=2,
                            end_char=3,
                            inner=SentSegTokenSeg(idx=1, orthography='ã«'),
                            attributes=SegAttribute(lemma='ã«', upos='ADP', xpos='åŠ©è©-æ ¼åŠ©è©', dependency=(0, 'case'), misc={'Reading': 'ãƒ‹', 'furigana_bracket': 'ã«', 'furigana_ruby': 'ã«', 'furigana_parentheses': 'ã«', 'hiragana_reading': 'ã«'}),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ã„ã£ãŸ',
                            start_char=3,
                            end_char=6,
                            inner=SentSegTokenSeg(idx=2, orthography='ã„ã£ãŸ'),
                            attributes=SegAttribute(
                                lemma='ã„ã†',
                                upos='VERB',
                                xpos='å‹•è©-éè‡ªç«‹å¯èƒ½',
                                dependency=(2, 'ROOT'),
                                misc={
                                    'Inflection': 'äº”æ®µ-ã‚«è¡Œ;é€£ç”¨å½¢-ä¿ƒéŸ³ä¾¿',
                                    'Reading': 'ã‚¤ãƒƒ',
                                    'furigana_bracket': 'ã„ã£',
                                    'furigana_ruby': 'ã„ã£',
                                    'furigana_parentheses': 'ã„ã£',
                                    'hiragana_reading': 'ã„ã£',
                                    'conjugation_base': 'ã„ã†',
                                    'conjugation_chain': [{'step': 1, 'form': 'Plain Past', 'result': 'ã„ã£ãŸ'}],
                                    'conjugation_sequence_length': 2,
                                    'conjugation_combined_text': 'ã„ã£ãŸ',
                                },
                            ),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ã€‚',
                            start_char=6,
                            end_char=7,
                            inner=SentSegPunctuationSeg(),
                            attributes=SegAttribute(lemma='ã€‚', upos='PUNCT', xpos='è£œåŠ©è¨˜å·-å¥ç‚¹', dependency=(2, 'punct'), misc={'Reading': 'ã€‚', 'furigana_bracket': 'ã€‚', 'furigana_ruby': 'ã€‚', 'furigana_parentheses': 'ã€‚', 'hiragana_reading': 'ã€‚'}),
                        ),
                    ]
                ),
            )
        ]
    )

    text = 'å…ˆç”ŸãŒãã†ã„ã£ãŸã€‚'  # è¨€ã†
    result = deinflector.unconjugate(verb)
    assert len(result) == snapshot(6)
    assert result == snapshot(
        [
            {'base': 'ã„ã†', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã¤', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã£ãŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã„ã£ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã„ã£ãŸ']}},
            {'base': 'ã„ã£ãŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã„ã£ãŸ']}},
        ]
    )

    result = parser.parse(text, ParserConfig("testing", {})).segments
    assert result == snapshot(
        [
            DocSegV2(
                text='å…ˆç”ŸãŒãã†ã„ã£ãŸã€‚',
                start_char=0,
                end_char=9,
                inner=DocSegSentence(
                    segments=[
                        SentSegV2(
                            sentence_idx=0,
                            text='å…ˆç”Ÿ',
                            start_char=0,
                            end_char=2,
                            inner=SentSegTokenSeg(idx=0, orthography='å…ˆç”Ÿ'),
                            attributes=SegAttribute(
                                lemma='å…ˆç”Ÿ',
                                upos='NOUN',
                                xpos='åè©-æ™®é€šåè©-ä¸€èˆ¬',
                                dependency=(3, 'nsubj'),
                                misc={'Reading': 'ã‚»ãƒ³ã‚»ã‚¤', 'furigana_bracket': 'å…ˆç”Ÿ[ã›ã‚“ã›ã„]', 'furigana_ruby': '<ruby>å…ˆç”Ÿ<rt>ã›ã‚“ã›ã„</rt></ruby>', 'furigana_parentheses': 'å…ˆç”Ÿ(ã›ã‚“ã›ã„)', 'hiragana_reading': 'ã›ã‚“ã›ã„'},
                            ),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ãŒ',
                            start_char=2,
                            end_char=3,
                            inner=SentSegTokenSeg(idx=1, orthography='ãŒ'),
                            attributes=SegAttribute(lemma='ãŒ', upos='ADP', xpos='åŠ©è©-æ ¼åŠ©è©', dependency=(0, 'case'), misc={'Reading': 'ã‚¬', 'furigana_bracket': 'ãŒ', 'furigana_ruby': 'ãŒ', 'furigana_parentheses': 'ãŒ', 'hiragana_reading': 'ãŒ'}),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ãã†',
                            start_char=3,
                            end_char=5,
                            inner=SentSegTokenSeg(idx=2, orthography='ãã†'),
                            attributes=SegAttribute(lemma='ãã†', upos='ADV', xpos='å‰¯è©', dependency=(3, 'advmod'), misc={'Reading': 'ã‚½ã‚¦', 'furigana_bracket': 'ãã†', 'furigana_ruby': 'ãã†', 'furigana_parentheses': 'ãã†', 'hiragana_reading': 'ãã†'}),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ã„ã£ãŸ',
                            start_char=5,
                            end_char=8,
                            inner=SentSegTokenSeg(idx=3, orthography='ã„ã£ãŸ'),
                            attributes=SegAttribute(
                                lemma='ã„ã†',
                                upos='VERB',
                                xpos='å‹•è©-ä¸€èˆ¬',
                                dependency=(3, 'ROOT'),
                                misc={
                                    'Inflection': 'äº”æ®µ-ãƒ¯ã‚¢è¡Œ;é€£ç”¨å½¢-ä¿ƒéŸ³ä¾¿',
                                    'Reading': 'ã‚¤ãƒƒ',
                                    'furigana_bracket': 'ã„ã£',
                                    'furigana_ruby': 'ã„ã£',
                                    'furigana_parentheses': 'ã„ã£',
                                    'hiragana_reading': 'ã„ã£',
                                    'conjugation_base': 'ã„ã†',
                                    'conjugation_chain': [{'step': 1, 'form': 'Plain Past', 'result': 'ã„ã£ãŸ'}],
                                    'conjugation_sequence_length': 2,
                                    'conjugation_combined_text': 'ã„ã£ãŸ',
                                },
                            ),
                        ),
                        SentSegV2(
                            sentence_idx=0,
                            text='ã€‚',
                            start_char=8,
                            end_char=9,
                            inner=SentSegPunctuationSeg(),
                            attributes=SegAttribute(lemma='ã€‚', upos='PUNCT', xpos='è£œåŠ©è¨˜å·-å¥ç‚¹', dependency=(3, 'punct'), misc={'Reading': 'ã€‚', 'furigana_bracket': 'ã€‚', 'furigana_ruby': 'ã€‚', 'furigana_parentheses': 'ã€‚', 'hiragana_reading': 'ã€‚'}),
                        ),
                    ]
                ),
            )
        ]
    )


def analyze_word(word: str) -> str:
    results = deinflector.unconjugate(word)

    if not results:
        return f"No results found for '{word}'"

    output = []
    output.append(f"Analysis of '{word}':")
    output.append(f"Dictionary form: {results[0]['base']}")
    output.append("")

    derivations = results[0]["derivation_sequence"]["derivations"]
    word_progression = results[0]["derivation_sequence"]["word_form_progression"]

    if not derivations:
        output.append("This is already in dictionary form.")
        return "\n".join(output)

    output.append("Derivation steps:")
    current_word = results[0]["base"]

    for i, (derivation_type, next_word) in enumerate(zip(derivations, word_progression)):

        output.append(f"{i+1}. {current_word} â†’ {next_word}")
        output.append(f"   Form: {derivation_type.value}")

        current_word = next_word
        output.append("")

    return "\n".join(output)


def test_analysis():
    """Test that grammar explanations are generated correctly for example words."""
    example_words = [
        "è¡Œãã¾ã™",
        "é£Ÿã¹ã‚‰ã‚Œãªã„",
        "èª­ã‚“ã§ã„ãŸ",
        "æ›¸ã‹ã‚Œã‚‹",
        "é£²ã¿ãŸã„",
    ]

    output_parts = []
    for word in example_words:
        analysis = analyze_word(word)
        output_parts.append(analysis)
        output_parts.append("-" * 50)
        output_parts.append("")

    if output_parts:
        output_parts = output_parts[:-2]

    combined_output = "\n".join(output_parts)
    assert combined_output == snapshot(
        """\
Analysis of 'è¡Œãã¾ã™':
Dictionary form: è¡Œã

Derivation steps:
1. è¡Œã â†’ è¡Œã
   Form: ã¾ã™ Stem

2. è¡Œã â†’ è¡Œãã¾ã™
   Form: ã¾ã™ Polite

--------------------------------------------------

Analysis of 'é£Ÿã¹ã‚‰ã‚Œãªã„':
Dictionary form: é£Ÿã¹ã‚‹

Derivation steps:
1. é£Ÿã¹ã‚‹ â†’ é£Ÿã¹ã‚‰ã‚Œã‚‹
   Form: Potential Or Passive Form

2. é£Ÿã¹ã‚‰ã‚Œã‚‹ â†’ é£Ÿã¹ã‚‰ã‚Œãªã„
   Form: ãªã„ Negative

--------------------------------------------------

Analysis of 'èª­ã‚“ã§ã„ãŸ':
Dictionary form: èª­ã‚€

Derivation steps:
1. èª­ã‚€ â†’ èª­ã‚“ã§
   Form: ã¦ãƒ»ã§ Form

2. èª­ã‚“ã§ â†’ èª­ã‚“ã§ã„ã‚‹
   Form: ã¦ã„ã‚‹ãƒ»ã§ã„ã‚‹ Continuing State/Result

3. èª­ã‚“ã§ã„ã‚‹ â†’ èª­ã‚“ã§ã„ãŸ
   Form: Plain Past

--------------------------------------------------

Analysis of 'æ›¸ã‹ã‚Œã‚‹':
Dictionary form: æ›¸ã

Derivation steps:
1. æ›¸ã â†’ æ›¸ã‹ã‚Œã‚‹
   Form: Passive Form

--------------------------------------------------

Analysis of 'é£²ã¿ãŸã„':
Dictionary form: é£²ã‚€

Derivation steps:
1. é£²ã‚€ â†’ é£²ã¿
   Form: ã¾ã™ Stem

2. é£²ã¿ â†’ é£²ã¿ãŸã„
   Form: ãŸã„ Want To Do
"""
    )


def test_conjugation_analyzer_filter_candidates():
    """Test that the conjugation analyzer can filter candidates based on tokenization."""
    # Test with ambiguous ã—ãŸ - should filter based on tokenization context

    # Parse the text to get real tokenization results (with conjugation analysis disabled)
    text = "ã—ãŸ"
    result = parser.parse(text, ParserConfig("enhanced_japanese", {"enable_conjugation_analysis": False}))

    # Extract token segments from parsed result
    token_segments = []
    for segment in result.segments:
        if hasattr(segment.inner, 'segments'):
            for sent_seg in segment.inner.segments:
                if isinstance(sent_seg.inner, SentSegTokenSeg):
                    token_segments.append(sent_seg)

    # Snapshot the tokenization result for verification
    tokenization_result = [(seg.text, seg.attributes.lemma, seg.attributes.upos, seg.attributes.xpos) for seg in token_segments]
    assert tokenization_result == snapshot([('ã—', 'ã™ã‚‹', 'VERB', 'å‹•è©-éè‡ªç«‹å¯èƒ½'), ('ãŸ', 'ãŸ', 'AUX', 'åŠ©å‹•è©')])

    # Get all candidates for ã—ãŸ
    candidates = deinflector.unconjugate("ã—ãŸ")

    # Snapshot original candidates for comparison
    assert candidates == snapshot(
        [
            {'base': 'ã™ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã—ãŸ']}},
            {'base': 'ã™', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã—ãŸ']}},
            {'base': 'ã—ãŸ', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ã—ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã—ãŸ']}},
            {'base': 'ã—ãŸã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ã—ãŸ']}},
        ]
    )

    # Filter based on tokenization (should prefer ã™ã‚‹ over other candidates)
    filtered = conjugation_analyzer._filter_candidates_by_tokenization(candidates, token_segments)

    # Snapshot filtered results showing which candidates were kept
    assert filtered == snapshot(
        [
            {'base': 'ã™ã‚‹', 'derivation_sequence': {'derivations': [WordType.PLAIN_PAST], 'word_form_progression': ['ã—ãŸ']}},
        ]
    )

    # Snapshot the length comparison to show filtering effectiveness
    candidate_count = {"original_candidates": len(candidates), "filtered_candidates": len(filtered)}
    assert candidate_count == snapshot({"original_candidates": 5, "filtered_candidates": 1})


def test_conjugation_analyzer_collect_sequence():
    """Test collecting conjugation sequences from tokenized segments."""
    # Test with "ç½®ã„ã¦ã„ã“ã†" (let's leave it there)
    text = "ç½®ã„ã¦ã„ã“ã†"
    # Parse with conjugation analysis disabled to get original separate tokens
    result = parser.parse(text, ParserConfig("enhanced_japanese", {"enable_conjugation_analysis": False}))

    # Extract token segments from parsed result
    token_segments = []
    for segment in result.segments:
        if hasattr(segment.inner, 'segments'):
            for sent_seg in segment.inner.segments:
                if isinstance(sent_seg.inner, SentSegTokenSeg):
                    token_segments.append(sent_seg)

    # Snapshot the tokenization result for verification (should have separate tokens)
    tokenization_result = [(seg.text, seg.attributes.lemma, seg.attributes.upos, seg.attributes.xpos) for seg in token_segments]
    assert tokenization_result == snapshot([('ç½®ã„', 'ç½®ã', 'VERB', 'å‹•è©-éè‡ªç«‹å¯èƒ½'), ('ã¦', 'ã¦', 'SCONJ', 'åŠ©è©-æ¥ç¶šåŠ©è©'), ('ã„ã“ã†', 'ã„ã', 'VERB', 'å‹•è©-éè‡ªç«‹å¯èƒ½')])

    # Collect sequence starting from the first verb
    sequence, combined_text = conjugation_analyzer._collect_conjugation_sequence(token_segments, 0)

    # Should collect the verb and the auxiliary tokens
    assert len(sequence) >= 1
    assert sequence[0].text == "ç½®ã„"

    # Snapshot the collected sequence
    sequence_result = [(seg.text, seg.attributes.upos) for seg in sequence]
    assert sequence_result == snapshot([('ç½®ã„', 'VERB'), ('ã¦', 'SCONJ'), ('ã„ã“ã†', 'VERB')])


def test_conjugation_chain_description():
    """Test creating human-readable conjugation chain descriptions."""
    # Test with a known conjugation
    candidates = deinflector.unconjugate("ç½®ã„ã¦ã„ã“ã†")

    # Snapshot all candidates to show what we're working with
    assert candidates == snapshot(
        [
            {'base': 'ç½®ã', 'derivation_sequence': {'derivations': [WordType.TE_FORM, WordType.TE_IKU, WordType.VOLITIONAL], 'word_form_progression': ['ç½®ã„ã¦', 'ç½®ã„ã¦ã„ã', 'ç½®ã„ã¦ã„ã“ã†']}},
            {'base': 'ç½®ã„ã‚‹', 'derivation_sequence': {'derivations': [WordType.TE_FORM, WordType.TE_IKU, WordType.VOLITIONAL], 'word_form_progression': ['ç½®ã„ã¦', 'ç½®ã„ã¦ã„ã', 'ç½®ã„ã¦ã„ã“ã†']}},
            {'base': 'ç½®ã„ã¦ã„ã', 'derivation_sequence': {'derivations': [WordType.VOLITIONAL], 'word_form_progression': ['ç½®ã„ã¦ã„ã“ã†']}},
            {'base': 'ç½®ã„ã¦ã„ã“ã†', 'derivation_sequence': {'derivations': [], 'word_form_progression': []}},
            {'base': 'ç½®ã„ã¦ã„ã“ã†ã‚‹', 'derivation_sequence': {'derivations': [WordType.MASU_STEM], 'word_form_progression': ['ç½®ã„ã¦ã„ã“ã†']}},
        ]
    )

    # Test with the best candidate (first one)
    best_candidate = candidates[0]
    chain = conjugation_analyzer._create_conjugation_chain_description(best_candidate["derivation_sequence"])

    # Snapshot the actual conjugation chain description showing all steps and structure
    assert chain == snapshot(
        [
            {'form': 'Volitional Form', 'result': 'ç½®ã„ã¦ã„ã“ã†', 'step': 1},
            {'form': 'ã¦ã„ããƒ»ã§ã„ã Gradual Change (Away From Speaker)', 'result': 'ç½®ã„ã¦ã„ã', 'step': 2},
            {'form': 'ã¦ãƒ»ã§ Form', 'result': 'ç½®ã„ã¦', 'step': 3},
        ]
    )

    # Snapshot chain length to make the count assertion explicit
    assert len(chain) == snapshot(3)


def test_japanese_parser_with_conjugation_analysis():
    """Test that JapaneseParser adds conjugation information to misc fields."""

    # Test with simple past tense
    text = "ã—ã¾ã£ãŸ"
    result = parser.parse(text, ParserConfig("enhanced_japanese", {}))

    # Extract all conjugation information from the result
    conjugation_info = []
    for segment in result.segments:
        if hasattr(segment.inner, 'segments'):
            for sent_seg in segment.inner.segments:
                if isinstance(sent_seg.inner, SentSegTokenSeg):
                    misc = sent_seg.attributes.misc
                    if "conjugation_base" in misc:
                        conjugation_info.append({"token": sent_seg.text, "base": misc["conjugation_base"], "chain": misc["conjugation_chain"], "combined_text": misc["conjugation_combined_text"]})

    # Snapshot the conjugation analysis results
    assert conjugation_info == snapshot([{'token': 'ã—ã¾ã£ãŸ', 'base': 'ã—ã¾ã†', 'chain': [{'step': 1, 'form': 'Plain Past', 'result': 'ã—ã¾ã£ãŸ'}], 'combined_text': 'ã—ã¾ã£ãŸ'}])


def test_japanese_parser_complex_conjugation():
    """Test parser with complex conjugation examples and snapshot their conjugation chains."""

    examples = [
        "ç½®ã„ã¦ã„ã“ã†",  # ç½®ã -> ã¦ form -> continuous change -> volitional
        "ä½œã£ã¦ãã‚Œã‚‹",  # ä½œã‚‹ -> ã¦ form -> benefit given
        "ç«‹ãŸãªã‹ã£ãŸ",  # ç«‹ã¤ -> negative -> past negative
        "ãªã£ã¦ã—ã¾ã£ãŸ",  # ãªã‚‹ -> ã¦ form -> completed action -> past
    ]

    all_conjugation_results = {}

    for text in examples:
        result = parser.parse(text, ParserConfig("enhanced_japanese", {}))

        # Extract conjugation information
        conjugation_info = []
        for segment in result.segments:
            if hasattr(segment.inner, 'segments'):
                for sent_seg in segment.inner.segments:
                    if isinstance(sent_seg.inner, SentSegTokenSeg):
                        misc = sent_seg.attributes.misc
                        if "conjugation_base" in misc:
                            conjugation_info.append({"token": sent_seg.text, "base": misc["conjugation_base"], "chain": misc["conjugation_chain"], "combined_text": misc["conjugation_combined_text"]})

        all_conjugation_results[text] = conjugation_info

    # Snapshot all conjugation analysis results
    assert all_conjugation_results == snapshot(
        {
            'ç½®ã„ã¦ã„ã“ã†': [
                {
                    'token': 'ç½®ã„ã¦ã„ã“ã†',
                    'base': 'ç½®ã',
                    'chain': [{'step': 1, 'form': 'Volitional Form', 'result': 'ç½®ã„ã¦ã„ã“ã†'}, {'step': 2, 'form': 'ã¦ã„ããƒ»ã§ã„ã Gradual Change (Away From Speaker)', 'result': 'ç½®ã„ã¦ã„ã'}, {'step': 3, 'form': 'ã¦ãƒ»ã§ Form', 'result': 'ç½®ã„ã¦'}],
                    'combined_text': 'ç½®ã„ã¦ã„ã“ã†',
                }
            ],
            'ä½œã£ã¦ãã‚Œã‚‹': [{'token': 'ä½œã£ã¦ãã‚Œã‚‹', 'base': 'ä½œã‚‹', 'chain': [{'step': 1, 'form': 'ãã‚Œã‚‹ To Give (Toward Speaker)', 'result': 'ä½œã£ã¦ãã‚Œã‚‹'}, {'step': 2, 'form': 'ã¦ãƒ»ã§ Form', 'result': 'ä½œã£ã¦'}], 'combined_text': 'ä½œã£ã¦ãã‚Œã‚‹'}],
            'ç«‹ãŸãªã‹ã£ãŸ': [{'token': 'ç«‹ãŸãªã‹ã£ãŸ', 'base': 'ç«‹ã¤', 'chain': [{'step': 1, 'form': 'Plain Past', 'result': 'ç«‹ãŸãªã‹ã£ãŸ'}, {'step': 2, 'form': 'ãªã„ Negative', 'result': 'ç«‹ãŸãªã„'}], 'combined_text': 'ç«‹ãŸãªã‹ã£ãŸ'}],
            'ãªã£ã¦ã—ã¾ã£ãŸ': [
                {
                    'token': 'ãªã£ã¦ã—ã¾ã£ãŸ',
                    'base': 'ãªã‚‹',
                    'chain': [{'step': 1, 'form': 'Plain Past', 'result': 'ãªã£ã¦ã—ã¾ã£ãŸ'}, {'step': 2, 'form': 'ã—ã¾ã† To Do Unfortunately ãƒ» To Do Completely', 'result': 'ãªã£ã¦ã—ã¾ã†'}, {'step': 3, 'form': 'ã¦ãƒ»ã§ Form', 'result': 'ãªã£ã¦'}],
                    'combined_text': 'ãªã£ã¦ã—ã¾ã£ãŸ',
                }
            ],
        }
    )


def test_conjugation_analyzer_integration():
    """Test the full integration of conjugation analysis."""

    # Test with "ãªã£ã¦ã—ã¾ã£ãŸ" (became/ended up becoming)
    text = "ãªã£ã¦ã—ã¾ã£ãŸ"
    result = parser.parse(text, ParserConfig("enhanced_japanese", {}))

    # Extract sentence segments
    sentence_segments = []
    for segment in result.segments:
        if hasattr(segment.inner, 'segments'):
            sentence_segments.extend(segment.inner.segments)

    # Filter to token segments only
    token_segments = [seg for seg in sentence_segments if isinstance(seg.inner, SentSegTokenSeg)]

    # Should have multiple tokens
    assert len(token_segments) > 0

    # Check if any token has conjugation information
    has_conjugation_info = any("conjugation_base" in seg.attributes.misc for seg in token_segments)

    # Note: This might not always be true depending on how the tokenizer splits the text
    # The test mainly ensures the integration doesn't crash


def test_example_conjugation_chains():
    """Test the specific examples mentioned in the requirements."""

    examples = [
        ("ç½®ã„ã¦ã„ã“ã†", "ç½®ã"),  # ç½®ã -> ã¦ form -> continuous change -> volitional
        ("ä½œã£ã¦ãã‚Œã‚‹", "ä½œã‚‹"),  # ä½œã‚‹ -> ã¦ form -> benefit given
        ("ã—ã¾ã£ãŸ", "ã—ã¾ã†"),  # ã—ã¾ã† -> past
        ("ç«‹ãŸãªã‹ã£ãŸ", "ç«‹ã¤"),  # ç«‹ã¤ -> negative -> past negative
        ("ãªã£ã¦ã—ã¾ã£ãŸ", "ãªã‚‹"),  # ãªã‚‹ -> ã¦ form -> completed action -> past
    ]

    for conjugated_form, expected_base in examples:
        # Test deinflection directly
        candidates = deinflector.unconjugate(conjugated_form)

        # Should have at least one candidate
        assert len(candidates) > 0, f"No candidates found for '{conjugated_form}'"

        # Check if expected base is among the candidates
        bases = [c["base"] for c in candidates]
        assert expected_base in bases, f"Expected base '{expected_base}' not found in {bases} for '{conjugated_form}'"

        # Test parser integration
        result = parser.parse(conjugated_form, ParserConfig("enhanced_japanese", {}))
        assert result.text == conjugated_form
